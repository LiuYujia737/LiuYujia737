{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "def load_tsv_data(file_path):\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                review = parts[0]\n",
    "                features = parts[1:]\n",
    "                for feature in features:\n",
    "                    feature_parts = feature.split()\n",
    "                    if len(feature_parts) >= 4:\n",
    "                        aspect_term_indices = feature_parts[0].split(',')\n",
    "                        sentiment = feature_parts[2]\n",
    "                        sentiment_term_indices = feature_parts[3].split(',')\n",
    "                        \n",
    "                        if aspect_term_indices[0] == '-1':\n",
    "                            aspect_term = 'NULL'\n",
    "                        else:\n",
    "                            start, end = int(aspect_term_indices[0]), int(aspect_term_indices[1])\n",
    "                            aspect_term = ' '.join(review.split()[start:end])\n",
    "                        \n",
    "                        if sentiment_term_indices[0] == '-1':\n",
    "                            sentiment_term = 'NULL'\n",
    "                        else:\n",
    "                            start, end = int(sentiment_term_indices[0]), int(sentiment_term_indices[1])\n",
    "                            sentiment_term = ' '.join(review.split()[start:end])\n",
    "                        \n",
    "                        data.append({\n",
    "                            'review': review,\n",
    "                            'aspect_term': aspect_term,\n",
    "                            'sentiment_term': sentiment_term,\n",
    "                            'sentiment': sentiment\n",
    "                        })\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line.strip()}\")\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def train_and_tune_model(X_train, y_train, X_dev, y_dev):\n",
    "    try:\n",
    "        # define the parameter grid\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto', 0.1, 1]\n",
    "        }\n",
    "        \n",
    "        # perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # get the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # evaluate the best model on the dev\n",
    "        dev_accuracy = best_model.score(X_dev, y_dev)\n",
    "        print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Development set accuracy: {dev_accuracy}\")\n",
    "        \n",
    "        return best_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training and tuning: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        return accuracy, report\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def extract_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # combine review, aspect term, and sentiment term into a single text\n",
    "        combined_text = f\"{review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(features)\n",
    "\n",
    "def baseline_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    \n",
    "    print(\"\\nExtracting features...\")\n",
    "    X_train = extract_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running baseline experiment...\n",
      "\n",
      "Extracting features...\n",
      "Encoding labels...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Development set accuracy: 0.8582375478927203\n",
      "Evaluating model...\n",
      "\n",
      "Baseline Experiment Results:\n",
      "Accuracy: 0.8482532751091703\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74       205\n",
      "           1       0.56      0.11      0.19        44\n",
      "           2       0.92      0.90      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.71      0.62      0.61       916\n",
      "weighted avg       0.85      0.85      0.84       916\n",
      "\n",
      "\n",
      "Baseline Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning baseline experiment...\")\n",
    "    baseline_classifier, baseline_le, baseline_accuracy, baseline_report = baseline_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nBaseline Experiment Results:\")\n",
    "    print(f\"Accuracy: {baseline_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(baseline_report)\n",
    "\n",
    "    print(\"\\nBaseline Experiments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import logging\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.data.path.append(\"D:\\\\nltk_data\")\n",
    "\n",
    "# cache file for storing ConceptNet data\n",
    "cache_file = 'conceptnet_cache.pkl'\n",
    "\n",
    "def load_cache():\n",
    "    global conceptnet_cache\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            conceptnet_cache = pickle.load(f)\n",
    "    else:\n",
    "        conceptnet_cache = {}\n",
    "\n",
    "def save_cache():\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(conceptnet_cache, f)\n",
    "\n",
    "def get_related_terms(word, max_retries=3, retry_delay=5):\n",
    "    if word in conceptnet_cache:\n",
    "        return conceptnet_cache[word]\n",
    "\n",
    "    url = f\"http://api.conceptnet.io/c/en/{word}?limit=50\"  # increase limit for more results\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            related_terms = set()  # store related terms in a set to avoid duplicates\n",
    "            for edge in data.get('edges', []):\n",
    "                rel = edge.get('rel', {}).get('label')\n",
    "                end = edge.get('end', {})\n",
    "                if isinstance(end, dict) and end.get('language') == 'en':\n",
    "                    label = end.get('label')\n",
    "                    # only keep related terms that are different from the original word\n",
    "                    if label and label != word and rel in ['IsA', 'RelatedTo', 'Synonym', 'HasA', 'PartOf', 'UsedFor', 'CapableOf']:\n",
    "                        related_terms.add(label)\n",
    "            conceptnet_cache[word] = list(related_terms)  # convert set to list before storing\n",
    "            return conceptnet_cache[word]\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Error fetching data for '{word}'. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for '{word}' after {max_retries} attempts.\")\n",
    "                logging.error(f\"Failed to fetch data for '{word}': {str(e)}\")\n",
    "                return []\n",
    "        except KeyError as e:\n",
    "            print(f\"Unexpected data structure for '{word}'. Skipping...\")\n",
    "            logging.error(f\"Unexpected data structure for '{word}': {str(e)}\\nData: {data}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing '{word}'. Skipping...\")\n",
    "            logging.error(f\"Unexpected error processing '{word}': {str(e)}\\nData: {data}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Development set accuracy: 0.7969348659003831\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.732532751091703\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.57      0.52       205\n",
      "           1       1.00      0.02      0.04        44\n",
      "           2       0.83      0.83      0.83       667\n",
      "\n",
      "    accuracy                           0.73       916\n",
      "   macro avg       0.77      0.47      0.46       916\n",
      "weighted avg       0.76      0.73      0.72       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # tokenize and tag words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # keep only nouns\n",
    "    nouns = [word for word, pos in tagged if pos.startswith('NN')]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    nouns = [noun for noun in nouns if noun not in stop_words]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "def enrich_text(text, max_related=5):\n",
    "    # get_related_terms can only enter one word, so preprocess_text first is necessary \n",
    "    words = preprocess_text(text)\n",
    "    enriched_words = []\n",
    "    for word in words:\n",
    "        enriched_words.append(word)\n",
    "        related_terms = get_related_terms(word)\n",
    "        # filter out words that are already in the text\n",
    "        filtered_terms = [term for term in related_terms if term not in words]\n",
    "        \n",
    "        enriched_words.extend(filtered_terms[:max_related])\n",
    "        # remian the original word\n",
    "    return ' '.join(enriched_words)\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # enrich the review text with related terms from ConceptNet\n",
    "        enriched_review = enrich_text(review)\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()  \n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 aspect_term or sentiment_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Development set accuracy: 0.8582375478927203\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8329694323144105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.71       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.90      0.90      0.90       667\n",
      "\n",
      "    accuracy                           0.83       916\n",
      "   macro avg       0.52      0.57      0.54       916\n",
      "weighted avg       0.80      0.83      0.82       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use aspect term if available, otherwise use sentiment term\n",
    "        primary_term = aspect_term if aspect_term else sentiment_term\n",
    "        enriched_terms = enrich_text(primary_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()  \n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 aspect_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Development set accuracy: 0.8582375478927203\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8329694323144105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.71       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.90      0.90      0.90       667\n",
      "\n",
      "    accuracy                           0.83       916\n",
      "   macro avg       0.52      0.57      0.54       916\n",
      "weighted avg       0.80      0.83      0.82       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use aspect term\n",
    "        enriched_terms = enrich_text(aspect_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()  \n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 sentiment_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Development set accuracy: 0.8544061302681992\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8482532751091703\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.92      0.91      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.53      0.58      0.55       916\n",
      "weighted avg       0.82      0.85      0.83       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()  \n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Results:\n",
      "Baseline Accuracy: 0.8482532751091703\n",
      "ConceptNet Accuracy: 0.8482532751091703\n",
      "Accuracy Improvement: 0.0\n",
      "\n",
      "Detailed Comparison:\n",
      "Baseline Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74       205\n",
      "           1       0.56      0.11      0.19        44\n",
      "           2       0.92      0.90      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.71      0.62      0.61       916\n",
      "weighted avg       0.85      0.85      0.84       916\n",
      "\n",
      "\n",
      "ConceptNet Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.92      0.91      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.53      0.58      0.55       916\n",
      "weighted avg       0.82      0.85      0.83       916\n",
      "\n",
      "\n",
      "Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"\\nComparison of Results:\")\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy}\")\n",
    "print(f\"ConceptNet Accuracy: {conceptnet_accuracy}\")\n",
    "print(f\"Accuracy Improvement: {conceptnet_accuracy - baseline_accuracy}\")\n",
    "\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(\"Baseline Classification Report:\")\n",
    "print(baseline_report)\n",
    "print(\"\\nConceptNet Classification Report:\")\n",
    "print(conceptnet_report)\n",
    "\n",
    "print(\"\\nExperiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 sentiment_term else aspect_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Development set accuracy: 0.8544061302681992\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8482532751091703\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.82      0.74       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.92      0.91      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.53      0.58      0.55       916\n",
      "weighted avg       0.82      0.85      0.83       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment term if available, otherwise use aspect term\n",
    "        primary_term = sentiment_term if sentiment_term else aspect_term\n",
    "        enriched_terms = enrich_text(primary_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()  \n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Performing feature selection...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Development set accuracy: 0.8773946360153256\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.851528384279476\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73       205\n",
      "           1       0.60      0.14      0.22        44\n",
      "           2       0.92      0.91      0.92       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.73      0.62      0.62       916\n",
      "weighted avg       0.85      0.85      0.84       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    # \n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(1000, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_dev_selected = selector.transform(X_dev)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train_selected, y_train, X_dev_selected, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test_selected, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Performing feature selection...\n",
      "Applying SMOTE for class balancing...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Development set accuracy: 0.8697318007662835\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8395196506550219\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71       205\n",
      "           1       0.47      0.18      0.26        44\n",
      "           2       0.91      0.90      0.91       667\n",
      "\n",
      "    accuracy                           0.84       916\n",
      "   macro avg       0.68      0.62      0.63       916\n",
      "weighted avg       0.84      0.84      0.83       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    # Optimal feature selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(1000, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_dev_selected = selector.transform(X_dev)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # SMOTE for class balancing\n",
    "    print(\"Applying SMOTE for class balancing...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_selected, y_train)\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train_resampled, y_train_resampled, X_dev_selected, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test_selected, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Performing feature selection...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'C': 0.1, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Development set accuracy: 0.8352490421455939\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8187772925764192\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.80      0.72       205\n",
      "           1       0.23      0.32      0.27        44\n",
      "           2       0.95      0.86      0.90       667\n",
      "\n",
      "    accuracy                           0.82       916\n",
      "   macro avg       0.61      0.66      0.63       916\n",
      "weighted avg       0.85      0.82      0.83       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "def train_and_tune_model(X_train, y_train, X_dev, y_dev):\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'class_weight': [None, 'balanced'] # add class_weight parameter, set to 'balanced' to account for class imbalance\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(SVC(probability=True), param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    dev_accuracy = best_model.score(X_dev, y_dev)\n",
    "    print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Development set accuracy: {dev_accuracy}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    # perform feature selection to reduce dimensionality \n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(1000, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_dev_selected = selector.transform(X_dev)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train_selected, y_train, X_dev_selected, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test_selected, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try different classifier\n",
    "\n",
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Performing feature selection...\n",
      "Training and tuning model...\n",
      "Best model parameters: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Development set accuracy: 0.8275862068965517\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8165938864628821\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.86      0.92      0.89       667\n",
      "\n",
      "    accuracy                           0.82       916\n",
      "   macro avg       0.51      0.52      0.52       916\n",
      "weighted avg       0.77      0.82      0.79       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_and_tune_model(X_train, y_train, X_dev, y_dev):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    dev_accuracy = best_model.score(X_dev, y_dev)\n",
    "    print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Development set accuracy: {dev_accuracy}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    # perform feature selection to reduce dimensionality \n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(1000, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_dev_selected = selector.transform(X_dev)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train_selected, y_train, X_dev_selected, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test_selected, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Performing feature selection...\n",
      "Training and tuning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.8}\n",
      "Development set accuracy: 0.842911877394636\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8504366812227074\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.83      0.75       205\n",
      "           1       0.50      0.02      0.04        44\n",
      "           2       0.91      0.91      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.70      0.59      0.57       916\n",
      "weighted avg       0.84      0.85      0.83       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_and_tune_model(X_train, y_train, X_dev, y_dev):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'), param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    dev_accuracy = best_model.score(X_dev, y_dev)\n",
    "    print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Development set accuracy: {dev_accuracy}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    # perform feature selection to reduce dimensionality \n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(1000, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_dev_selected = selector.transform(X_dev)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train_selected, y_train, X_dev_selected, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test_selected, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ConceptNet experiment...\n",
      "Preprocessing data...\n",
      "\n",
      "Extracting features with ConceptNet enrichment...\n",
      "Encoding labels...\n",
      "Performing feature selection...\n",
      "Training and tuning model...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 2484, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score -1.220480\n",
      "[LightGBM] [Info] Start training from score -3.263749\n",
      "[LightGBM] [Info] Start training from score -0.405465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best model parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 20, 'n_estimators': 300, 'subsample': 0.8}\n",
      "Development set accuracy: 0.8467432950191571\n",
      "Evaluating model...\n",
      "\n",
      "Conceptnet Experiment Results:\n",
      "Accuracy: 0.8504366812227074\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.82      0.75       205\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.91      0.92      0.91       667\n",
      "\n",
      "    accuracy                           0.85       916\n",
      "   macro avg       0.53      0.58      0.55       916\n",
      "weighted avg       0.82      0.85      0.83       916\n",
      "\n",
      "\n",
      "Conceptnet Experiments completed.\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_and_tune_model(X_train, y_train, X_dev, y_dev):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [-1, 10, 20],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(LGBMClassifier(random_state=42), param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    dev_accuracy = best_model.score(X_dev, y_dev)\n",
    "    print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Development set accuracy: {dev_accuracy}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def enrich_text(primary_term, max_related=5):\n",
    "    terms = primary_term.split()\n",
    "    \n",
    "    related_terms = []\n",
    "    for term in terms:\n",
    "        related_terms.extend(get_related_terms(term))\n",
    "    \n",
    "    filtered_terms = [term for term in related_terms if term not in terms]\n",
    "    \n",
    "    return ' '.join(filtered_terms[:max_related])\n",
    "\n",
    "def extract_conceptnet_features(reviews, aspect_terms, sentiment_terms, model, tokenizer):\n",
    "    features = []\n",
    "    for review, aspect_term, sentiment_term in zip(reviews, aspect_terms, sentiment_terms):\n",
    "        # only use sentiment_term\n",
    "        enriched_terms = enrich_text(sentiment_term)\n",
    "        enriched_review = review + ' ' + enriched_terms\n",
    "        \n",
    "        combined_text = f\"{enriched_review} [SEP] {aspect_term} [SEP] {sentiment_term}\"\n",
    "        \n",
    "        inputs = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        features.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        \n",
    "        save_cache()\n",
    "    \n",
    "    save_cache()  \n",
    "    return np.array(features)\n",
    "\n",
    "def conceptnet_experiment(train_data, dev_data, test_data, model, tokenizer):\n",
    "    load_cache()\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    print(\"\\nExtracting features with ConceptNet enrichment...\")\n",
    "    X_train = extract_conceptnet_features(train_data['review'], train_data['aspect_term'], train_data['sentiment_term'], model, tokenizer)\n",
    "    X_dev = extract_conceptnet_features(dev_data['review'], dev_data['aspect_term'], dev_data['sentiment_term'], model, tokenizer)\n",
    "    X_test = extract_conceptnet_features(test_data['review'], test_data['aspect_term'], test_data['sentiment_term'], model, tokenizer)\n",
    "    \n",
    "    print(\"Encoding labels...\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data['sentiment'].tolist() + dev_data['sentiment'].tolist() + test_data['sentiment'].tolist())\n",
    "    y_train = le.transform(train_data['sentiment'])\n",
    "    y_dev = le.transform(dev_data['sentiment'])\n",
    "    y_test = le.transform(test_data['sentiment'])\n",
    "    \n",
    "    # perform feature selection to reduce dimensionality \n",
    "    print(\"Performing feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(1000, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_dev_selected = selector.transform(X_dev)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    print(\"Training and tuning model...\")\n",
    "    best_classifier = train_and_tune_model(X_train_selected, y_train, X_dev_selected, y_dev)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, report = evaluate_model(best_classifier, X_test_selected, y_test)\n",
    "    \n",
    "    return best_classifier, le, accuracy, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bert_model = BertModel.from_pretrained(r'D:\\AIProject\\Bert\\model')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(r'D:\\AIProject\\Bert\\tokenizer')\n",
    "\n",
    "    # restuarant\n",
    "    rest_train_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_train.tsv')\n",
    "    rest_test_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_test.tsv')\n",
    "    rest_dev_data = load_tsv_data(r'D:\\AIProject\\data\\restaurant\\rest16_quad_dev.tsv')\n",
    "    \n",
    "    print(\"\\nRunning ConceptNet experiment...\")\n",
    "    conceptnet_classifier, conceptnet_le, conceptnet_accuracy, conceptnet_report = conceptnet_experiment(rest_train_data, rest_dev_data, rest_test_data, bert_model, bert_tokenizer)\n",
    "    \n",
    "    print(\"\\nConceptnet Experiment Results:\")\n",
    "    print(f\"Accuracy: {conceptnet_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(conceptnet_report)\n",
    "\n",
    "    print(\"\\nConceptnet Experiments completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
